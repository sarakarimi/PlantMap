<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="None">
        
        
        <link rel="shortcut icon" href="img/favicon.ico">
        <title>PlantMap</title>
        <link href="css/bootstrap.min.css" rel="stylesheet">
        <link href="css/fontawesome.min.css" rel="stylesheet">
        <link href="css/brands.min.css" rel="stylesheet">
        <link href="css/solid.min.css" rel="stylesheet">
        <link href="css/v4-font-face.min.css" rel="stylesheet">
        <link href="css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <link href="css/styles.css" rel="stylesheet">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body class="homepage">
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href=".">PlantMap</a>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#plantmap" class="nav-link">PlantMap</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#github-repo" class="nav-link">Github Repo</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#setup-environment" class="nav-link">Setup Environment</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#scalable-solution" class="nav-link">Scalable solution</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#machine-learning-methods" class="nav-link">Machine Learning Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#datasets" class="nav-link">Datasets</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#experiments-results" class="nav-link">Experiments &amp; Results</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#conclusions" class="nav-link">Conclusions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#future-work" class="nav-link">Future Work</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#individual-contributions" class="nav-link">Individual Contributions</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#references" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><link rel="stylesheet" href="extra.css" /></p>
<h1 id="plantmap">PlantMap</h1>
<p><strong>Federated learning for segmentation, detection, and classification of weed species in aerial images taken from farm fields</strong></p>
<h2 id="github-repo">Github Repo</h2>
<p><a href="https://github.com/sarakarimi/PlantMap/blob/main/README.md">https://github.com/sarakarimi/PlantMap/blob/main/README.md</a></p>
<h2 id="setup-environment">Setup Environment</h2>
<pre><code>conda create --prefix &lt;path to new conda env&gt; python=3.12
conda activate &lt;path or name&gt;
conda install -c conda-forge huggingface_hub
cd /tmp # or any empty directory
git clone https://github.com/facebookresearch/sam2.git
cd sam2
pip install torch torchvision numpy
pip install -e .
pip install -e &quot;.[notebooks]&quot;
pip install scipy

</code></pre>
<h4 id="setup-the-finetune-environment-using-fedn">Setup the finetune environment using FEDn</h4>
<p>To get familiar with it, we were using <a href="https://github.com/astral-sh/uv">uv</a> rather than more common virtual environments like conda.
Install it using <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code> (on Linux and macOS).
No additional steps are needed to prepare the environment there.
In hindsight, we should set up the entire repository using uv.</p>
<h3 id="project-structure">Project Structure</h3>
<pre><code>├── assests
├── fedn_supervised
│   └── client
│       └── config
├── finetune_mae
│   └── src
│       ├── data
│       ├── models
│       └── utils
├── pretrain_clip
│   └── config
│       ├── model
│       └── training
├── sam_clip                # Code for automatic annotation and labeling of raw images using SAM+CLIP
│   ├── config              # Model hyperparameters &amp; configs
│   ├── data                # Prompt tokens
│   │   └── resources
│   ├── models              # SAM and clip model integratetion
│   │   ├── clip
│   │   ├── detector
│   │   └── sam
│   ├── pretrained          # Pre-trained models' checkpoints
│   │   ├── clip
│   │   └── sam
│   └── utils
└── unsupervised_classification
</code></pre>
<h4 id="create-masks">Create Masks</h4>
<p>We experienced with that during the early project stage.</p>
<pre><code># Place all images in data/Hyperlapse
mkdir masks
mkdir test_results
python scripts/huggingface_sam_test.py

</code></pre>
<h4 id="create-flower-dataset-for-unsupervised-classification">Create Flower Dataset for Unsupervised Classification</h4>
<pre><code>mkdir masks
conda activate &lt;path or name&gt;
python unsupervised_classification/create_all_flowers.py
</code></pre>
<h4 id="similarity-matching">Similarity Matching</h4>
<p>Due to laziness, the current code does only compare all flowers of a single picture.</p>
<pre><code># Run the &quot;Create Flower Dataset for Unsupervised Classification&quot; step
mkdir similarities

conda activate &lt;path or name&gt;
python unsupervised_classification/feature_matching.py
</code></pre>
<h4 id="running-fine-tuning-locally-with-different-clip-configs">Running fine-tuning locally with different CLIP configs</h4>
<p>First enter the <code>pretrain_clip</code> directory and ensure that you have <code>uv</code> installed. <strong>NOTE:</strong> This is only confirmed to work on a Linux machine (with or without GPU). MacOS is not supported and Windows is untested.</p>
<p>All the configurations for the model are contained within the <code>config</code> directory, both for pre-training on EWD and fine-tuning on the main dataset. There is a base configuration, with specific presets for the training and model parameters. In order to run the fine-tuning with the default parameters (base CLIP model with parameters from Ray Tune and using the PlantMap dataset), simply run:</p>
<pre><code>uv run finetune_model.py
</code></pre>
<p>If you wish to run the model with the CLIP model set to the pre-trained classification model, then run:</p>
<pre><code>uv run finetune_model.py model=classifier training=classifier
</code></pre>
<p>If you wish to do the same for the contrastive pre-trained model, then run:</p>
<pre><code>uv run finetune_model.py model=contrastive training=contrastive
</code></pre>
<p>Add <code>pretraining_</code> to the start of both config names if you want to run pre-training on EWD instead of fine-tuning. If you wish to change a specific parameter (for example the number of epochs to train for, the value of dropout, or the batch size) then you can run:</p>
<pre><code>uv run finetune_model.py training.epochs=10 training.batch_size=128 model.dropout=0.2
</code></pre>
<p>This works the same whether you use the base CLIP model, or load in one of the other four models.</p>
<h3 id="project-members">Project members:</h3>
<p>Derya Akbaba - Linkoping University <br>
Sofia Andersson - Lund University <br>
Sara karimi - KTH Royal Institute of Technology <br>
Markus Fritzsche - Linkoping University <br>
Xavante Erickson - Lund University and Ericsson<br></p>
<p><a href="https://docs.google.com/presentation/d/1hjCFfOYfRXobQbDJ9j70Z-hZU9LCGI1fGsSZFg5RXzU/edit?usp=sharing">Slides</a></p>
<h2 id="introduction">Introduction</h2>
<p>Understanding and managing the biodiversity of farming fields is crucial for sustainable agriculture and efficient resource utilization.
Having accurate information about the composition of wildflowers play an important role in managing the biodiversity achieving more sustainable and efficient farming practices.
But given that wildflowers are small, sparsely scattered in large areas, and have short blooming cycles, tracking the composition using traditional methods are challenging.</p>
<p>This project aims to create a "plant map" of farm fields by identifying specific plant species at various coordinates, provided using aerial imaging captured by drones.
The resulting map will provide critical insights into plant distribution, enabling farmers to optimize pesticide application and tailor soil mineral combinations for enhanced crop growth.
Furthermore, the timing of cattle grazing in spring, when critical flora has flowered, significantly impacts both cattle well-being and environmental balance.
So efficiently identify key flora across vast grazing areas and providing real-time statistics to farmers can further reduce the costs and optimizes the release of cattle for grazing which further contribute to efficient farming.
To help this effort, we use machine learning approaches leveraging state-of-the-art computer vision techniques and pre-trained vision models and fine-tune them on datasets collected from farms. To allow farmers' data privacy we employ a federated learning approach where each farmer can have local trainings without the need to share data globally.
We build on prior work by Schouten et al. [1] who contribute an expert-annotated dataset of wildflower images from the Netherlands. We hope can be small contribution towards smarter and more sustainable farming.</p>
<h2 id="scalable-solution">Scalable solution</h2>
<p>The main scalable solution in this project is built upon a federated learning architecture, enabling efficient training across distributed datasets while preserving data privacy.
To support this approach, we leverage the FedN tool, a framework designed for federated learning applications. Below, we provide a brief introduction to federated learning and the FedN tool.
The actual fine-tuning model is also built upon the PyTorch Lightning infrastructure, which allows for the possibility of distributed learning, as well as scalability, without altering the model architecture.</p>
<h3 id="federated-learning-and-fedn">Federated Learning and FedN</h3>
<p>Federated learning (FL) is a decentralized approach to machine learning that allows multiple parties to collaboratively train a shared model without sharing their raw data. This technique is particularly valuable in scenarios where data privacy, security, or locality is critical, such as healthcare, finance, and agriculture.</p>
<h4 id="how-federated-learning-works"><strong>How Federated Learning Works</strong></h4>
<ol>
<li><strong>Local Training</strong>: Each participating client (e.g., farmers, edge devices) trains a local model using its private data. This training process occurs independently and securely on each client’s device.</li>
<li><strong>Model Updates</strong>: Once training is complete, each client sends only the model updates (e.g., weights, gradients) to a central server. Importantly, raw data remains on the client’s device, ensuring privacy.</li>
<li><strong>Global Aggregation</strong>: The central server aggregates the updates from all participating clients to create a global model. This global model is then shared back with the clients for the next round of training.</li>
<li><strong>Iterative Process</strong>: Steps 1–3 are repeated for several rounds until the global model converges to a satisfactory performance level.</li>
</ol>
<p>Federated learning comes with advantages such as <strong>(i) privacy</strong>: as sensitive data never leaves the client’s device, reducing the risk of data breaches, <strong>(ii) scalability</strong>: as it can scale to millions of devices or participants, enabling collaborative training on massive datasets, and <strong>(iii) personalization</strong>: as clients can fine-tune the global model locally to adapt it to their specific data distribution.</p>
<h4 id="aggregation-methods-in-federated-learning"><strong>Aggregation Methods in Federated Learning</strong></h4>
<p>A critical aspect of FL is the aggregation of model updates to ensure the global model improves with each round. Common aggregation methods include:</p>
<ul>
<li><strong>Federated Averaging (FedAvg)</strong>: A simple yet effective method introduced by Sun et al. [2], where the global model is updated by averaging the weights or gradients received from clients, weighted by the size of each client's dataset.</li>
<li><strong>Gradient Aggregation</strong>: In scenarios where gradients are shared, these can be aggregated directly to update the global model.</li>
<li><strong>Adaptive Aggregation</strong>: Advanced methods that account for heterogeneity in client data, ensuring that clients with diverse data distributions contribute effectively to the global model.</li>
</ul>
<h4 id="fedn-for-federated-weed-detection"><strong>FedN for Federated Weed Detection</strong></h4>
<p>In this project, we leverage <a href="https://www.scaleoutsystems.com/framework"><strong>FedN</strong></a>, a robust federated learning framework designed for scalable and efficient model training. Using FedN, we aggregate locally trained models from farmers to create a unified global model for weed detection and classification. Each farmer's model is trained on their specific annotated data (e.g., clovers or chamomiles), and the aggregated model benefits from the diverse local datasets while respecting data privacy.
This approach allows us to build a high-performing foundation model for farm weed detection without compromising individual data security.</p>
<p style="text-align: center;">
<img src="img/federated.png" width="60%"> 
</p>

<h2 id="machine-learning-methods">Machine Learning Methods</h2>
<p>Since the solution entails performing <strong>image segmentation and object detection/classification</strong> tasks on images, the following sections detail the machine learning approaches employed for each specific task.</p>
<h3 id="segmentation-of-images">Segmentation of images</h3>
<p>Given a collection of raw images, the first step is to perform segmentation to identify and extract potentially interesting regions that contain objects within the images.
To achieve this, we have explored two distinct approaches: one supervised and the other unsupervised, which we detail further below.</p>
<ol>
<li>
<p><strong>Unsupervised Segmentation</strong> : Segments images into meaningful regions using semantic and instance mask generation with no labeled data required for further training or fine-tuning of the model (like <a href="https://github.com/u2seg/U2Seg">U2Seg (Unsupervised Universal Image Segmentation)</a>).</p>
</li>
<li>
<p><strong>Supervised Segmentation</strong> : Uses the Segment Anything Model (SAM) for the segmentation task. Training or fine-tuning of the SAM model requires labeled data</p>
</li>
</ol>
<h4 id="supervised-segmentation">Supervised Segmentation</h4>
<p>The <a href="https://github.com/facebookresearch/segment-anything">Segment Anything Model (SAM)</a> [3] is a foundation model for image segmentation, designed to handle a wide variety of segmentation tasks without the need for task-specific fine-tuning.
At its core, SAM uses a vision transformer (ViT) backbone to extract rich, hierarchical features from an image.
These features are then processed by a prompt encoder that allows users to provide inputs in the form of points, bounding boxes, or free-form text, specifying the region of interest to segment.
SAM combines these prompts with the image features to predict masks through a lightweight mask decoder.</p>
<p style="text-align: center;">
  <img src="img/sam.png" alt="Overview of SAM" width="70%">
</p>

<h3 id="classification-of-objects-in-the-images">Classification of objects in the images</h3>
<p>With the segmentations in place, we explore two approaches for the classification task:</p>
<ol>
<li>
<p><strong>Unsupervised Classification</strong> : This method relies on template matching to group segments of the original image that match with a given example without requiring labeled data.</p>
</li>
<li>
<p><strong>Supervised Classification</strong> : This approach utilizes the pre-trained CLIP (Contrastive Language–Image Pretraining) model, leveraging its powerful multi-modal capabilities to classify segments based on learned visual and the provided textual prompts.</p>
</li>
</ol>
<h4 id="semi-un-supervised-classification">(Semi-/Un-)supervised Classification</h4>
<p>We use SAM in order to get flower images without background as this is usually pretty accurate using a pretrained SAM2 model.
For every possible wildflower, we select one candidate as reference image.
We finetune a masked-autoencoder, discard the decoder and use the encoder part of the model to retrieve image features.</p>
<p>To train the model, we chose to compare two different approaches, <a href="https://arxiv.org/abs/2006.07733">BYOL</a> [5] and <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> [6], both methods for learning visual representations.
An encoder model maps each image into a vector. By normalizing the vector (unit vector) and comparing them using cosine-similarity, we get a probability of both images belonging to the same class or not.</p>
<h5 id="challenges">Challenges</h5>
<ul>
<li>The number of distinct flower classes is comparably low compared to the number of overall images</li>
<li>Just by looking on the raw data, it is clear that the dataset is not evenly distributed (class imbalance)</li>
<li>Many flowers of different classes look similar, e.g., all flowers with white blossoms.</li>
<li>SAM is not perfect, i.e., it predicts false positives</li>
</ul>
<h4 id="semi-un-supervised-classification_1">(Semi-/Un-)supervised Classification</h4>
<p>We use SAM in order to get flower images without background as this is usually pretty accurate using a pretrained SAM2 model.</p>
<p>For every possible wildflower, we select one candidate as reference image.
We finetune a masked-autoencoder, discard the decoder and use the encoder part of the model to retrieve image features.</p>
<p>To train the model, we chose to compare two different approaches, <a href="https://arxiv.org/abs/2006.07733">BYOL</a> and <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, both methods for learning visual representations.
An encoder model maps each image into a vector. By normalizing the vector (unit vector) and comparing them using cosine-similarity, we get a probability of both images belonging to the same class or not.</p>
<p style="text-align: center;">
<img src="img/unsupervised_classification.png" width="70%"> 
</p>

<h5 id="challenges_1">Challenges</h5>
<ul>
<li>The number of distinct flower classes is comparably low compared to the number of overall images</li>
<li>Just by looking on the raw data, it is clear that the dataset is not evenly distributed, i.e., unbalanced.</li>
<li>Many flowers of different classes look similar, e.g., all flowers with white blossoms.</li>
<li>SAM is not perfect, i.e., it predicts false positives and</li>
</ul>
<h4 id="supervised-classification">Supervised Classification</h4>
<p><a href="https://github.com/openai/CLIP">CLIP (Contrastive Language–Image Pretraining)</a> [4] is a multimodal model developed by OpenAI that learns to associate images and text through a contrastive learning framework.
It uses two separate neural networks one for images (a vision transformer or CNN) and one for text (transformer-based language model).
These networks encode images and text into a shared embedding space.
During training, CLIP is presented with image-text pairs and learns to align their embeddings, so that the embedding of an image is close to its corresponding text description and far from unrelated ones.
This enables CLIP to perform zero-shot tasks: it can recognize and classify images based on textual descriptions without requiring fine-tuning on specific datasets.</p>
<p style="text-align: center;">
<img src="img/clip1.png" width="30%"> 
<img src="img/clip2.png" width="32.5%"> 
</p>

<h3 id="pre-training">Pre-training</h3>
<p>Using the <a href="https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/U4VQJ6"><strong>Eindhoven Wildflower Dataset (EDW)</strong></a> to fine-tune the CLIP model on a variety of flowers, the theory is that this might improve performance on the target dataset, as the CLIP model would have seen many more images of flowers than those available in the dataset.
Even if the labels do not overlap fully, they should in theory be close enough in embedding space to hopefully provide the model with a better starting point.
The models were trained in two ways:</p>
<ol>
<li><strong>Cross-categorical entropy.</strong> The CLIP model parameters were trained along with a classifier head for the EWD.</li>
<li><strong>Supervised Contrastive Loss.</strong> This is the method that the CLIP model was originally trained with. Images and labels for the EWD were fed into the model and the contrastive loss was then used to update the CLIP weights.</li>
</ol>
<p>The best hyperparameters for each of the models were found using Optuna Search with Ray Tune. This search explored batch size, learning rate, and optimizers. The optimizers were AdaDelta, AdamW, and Stochastic Gradient Descent (SGD) with Nesterov momentum. All setups utilized a learning rate scheduler, with <a href="https://arxiv.org/abs/1608.03983">SGD using a cosine annealing scheduler with warm restarts</a> [7] and the others lowering learning rate by a factor of 10 after 10 epochs of stagnation.</p>
<p>After this first step, the best model for each type of training was found, and their weights uploaded to HuggingFace. These were then used as a starting point to see how the model performed on the true dataset and whether the pre-training helped, or if the base CLIP model has the same performance. For this part of the project, the CLIP model weights were frozen and the classification head was the only part of the model being trained. The classification head had Kaiming-initialized weights and bias set to zero to start off with, which was particularly important for the model trained with CCE, as the original training head does not have the same amount of classes as the current head. For this case, the entire classification head was re-initialized, with the hope that most of the learning had been done by the CLIP model. The results of this training can be seen in the table further down.</p>
<h2 id="datasets">Datasets</h2>
<p>Since we started with an unannotated dataset of raw images, it was necessary to first annotate and label the dataset to generate training data required for training an image object detection and classification model. To achieve this, we used the pre-trained SAM+CLIP approach described in the <strong>Method</strong> section.
Specifically, the raw images were fed into the <strong>SAM</strong> model, which performed segmentation to identify distinct regions in the images.
These segmented regions were then passed to the <strong>CLIP</strong> model for classification. The CLIP model was provided with a list of textual prompts representing the labels of existing weed species, enabling it to classify each segment according to the specified labels.
The resulting labeled dataset has over 2000 samples with labels for four weed species <strong>Daisy, Yarrow, Dandelion, and Red clover</strong>. The dataset is published in the HuggingFace dataset repository <a href="https://huggingface.co/datasets/sarakarimi30/PlantMap">PlantMap</a>.</p>
<p style="text-align: center;">
<img src="img/dataset_Annotation.png" width="60%"> 
</p>

<p>As detailed in the <strong>Pre-training</strong> section, to develop a base model better suited for detection and classification on the above-mentioned dataset, we pre-trained our model on a similar dataset, the EWD dataset. This dataset contains 2,002 high-resolution annotated images of wildflowers, providing a robust starting point for training.</p>
<h2 id="experiments-results">Experiments &amp; Results</h2>
<h3 id="results-of-single-machine-training">Results of single machine training</h3>
<table>
<thead>
<tr>
<th>Max accuracy</th>
<th>Model</th>
<th>Optimizer</th>
<th>Learning rate</th>
<th>Batch size</th>
<th>Dropout</th>
<th>First epoch acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>89.3232</td>
<td>Categorical</td>
<td>SGD</td>
<td>0.02</td>
<td>32</td>
<td>0</td>
<td>88.44</td>
</tr>
<tr>
<td>91.411</td>
<td>Base CLIP</td>
<td>SGD</td>
<td>0.02</td>
<td>32</td>
<td>0</td>
<td>87.68</td>
</tr>
<tr>
<td>90.2979</td>
<td>Categorical</td>
<td>SGD</td>
<td>0.001</td>
<td>32</td>
<td>0</td>
<td>85.32</td>
</tr>
<tr>
<td>85.4972</td>
<td>Base CLIP</td>
<td>SGD</td>
<td>0.001</td>
<td>32</td>
<td>0</td>
<td>66.51</td>
</tr>
<tr>
<td>86.5553</td>
<td>Contrast</td>
<td>AdaDelta</td>
<td>0.04</td>
<td>32</td>
<td>0.2</td>
<td>72.17</td>
</tr>
<tr>
<td>86.77</td>
<td>Contrast</td>
<td>AdaDelta</td>
<td>0.04</td>
<td>32</td>
<td>0</td>
<td>72.88</td>
</tr>
<tr>
<td>88.67</td>
<td>Contrast</td>
<td>AdaDelta</td>
<td>0.4</td>
<td>32</td>
<td>0</td>
<td>86.78</td>
</tr>
<tr>
<td>90.48</td>
<td>Contrast</td>
<td>AdaDelta</td>
<td>4.5</td>
<td>32</td>
<td>0</td>
<td>85.61</td>
</tr>
<tr>
<td>89.76</td>
<td>Contrast</td>
<td>AdamW</td>
<td>0.008</td>
<td>32</td>
<td>0.1</td>
<td>87.04</td>
</tr>
<tr>
<td>88.61</td>
<td>Contrast</td>
<td>AdamW</td>
<td>0.01</td>
<td>32</td>
<td>0</td>
<td>85.55</td>
</tr>
</tbody>
</table>
<p>These are the results of the fine-tuning on a single machine for a variety of hyperparameters. Originally, only the top two sets of parameters per model were going to be investigated (as found by Ray Tune), but there was evidence of overfitting for the contrastive model, which led to further investigation into both learning rates and dropout rates. Both max accuracy and the first epoch accuracy were calculated, as some models took a very long time to reach their max accuracy, and federated learning may prefer models that learn more quickly.</p>
<h3 id="results-of-federated-learning">Results of federated learning</h3>
<p style="text-align: center;">
<img src="img/fedAvg_train.png" width="49%"> 
<img src="img/fedAvg_test.png" width="50%"> 
<img src="img/fedAdam_train.png" width="49%"> 
<img src="img/fedAdam_test.png" width="50%" > 
</p>

<h2 id="conclusions">Conclusions</h2>
<p><strong>Segmentation</strong> <br></p>
<ul>
<li>The SAM model is good base model to use for segmentation but it needs a lot of hyperparameter tuning to adapt to unseen datasets</li>
</ul>
<p><strong>Pre-training on EWD</strong> <br></p>
<ul>
<li>Appears to increase accuracy compared to base CLIP model <br></li>
<li>Overall better first epoch accuracy <br></li>
<li>Particularly relevant at lower learning rates <br></li>
<li>Higher learning rates show barely any learning <br></li>
</ul>
<p><strong>Federated learning</strong></p>
<ul>
<li>Federated learning improves performance while maninting data privacy <br></li>
<li>FedN needs more documentations, more robust operations, better server looks and better fault tolerance! <br></li>
</ul>
<h2 id="future-work">Future Work</h2>
<p><strong>Dataset, Segmentation, &amp; Annotation</strong> <br></p>
<ul>
<li>Quality of images made it hard to detect small flower species (only daisy and yarrow was used) <br></li>
<li>Better dataset annotation using pre-trained models to cover more species <br></li>
<li>Use of segmentation models with editing capabilities to build a better dataset <br></li>
<li>Exploration of unsupervised segmentation methods like U2seg <br></li>
</ul>
<p><strong>‘Real world’ application</strong> <br></p>
<ul>
<li>Continual learning and human feedback <br></li>
<li>Including, exploring different types of federated aggregation <br></li>
<li>GUI to support farmers with varying technical skills <br></li>
<li>Test whether model can be app-ified <br></li>
</ul>
<h2 id="individual-contributions">Individual Contributions</h2>
<p><strong>Sara Karimi:</strong> Worked on dataset creation (uploaded to HuggingFace) including segmentation and labeling of the flower species in images using a combined SAM+CLIP approach. Conducted an early evaluation of LabelStudio with an integrated SAM model but opted against it to ensure more robust and direct access to SAM functionalities.</p>
<p><strong>Sofia Andersson:</strong> Pre-trained the CLIP models on the EWD using both a contrastive and classification approach. Wrote the fine-tuning script for the models using PyTorch Lightning with a flexible Hydra config to be able to run many different variations of the models. Optimized hyperparameters for all pre-trained and fine-tuned models using Optuna Search with Ray Tune. Ran the one-machine fine-tuning tests using the base CLIP model and the pre-trained CLIP models.</p>
<p><strong>Markus Fritzsche:</strong> Observing how methods like <a href="https://arxiv.org/abs/2002.05709">SimCLR</a>, <a href="https://arxiv.org/abs/1911.05722">MoCo</a>, and <a href="https://arxiv.org/abs/2006.07733">BYOL</a> can be used to fine-tune pre-trained vision transformer encoders to retrieve class dependent image features with a high cosine similarity score if two images belong to the same flower class and a low score otherwise.
In addition, taking Sofia's work as a base to make it FEDn compatible, including distributing the dataset among all clients.</p>
<p><strong>Xavante Erickson:</strong> Prepared and converted EWD to huggingface compatible format for Sofia Andersson's and Markus Fritzsche's work on fine-tuning. Explored unsupervised learning for panoptic segmentation using the PlantMap dataset. Assisted with FEDn development.</p>
<p><strong>Derya Akbaba:</strong> Created initial dataset (uploaded to HuggingFace) including raw images used later by Sara. Created initial Docker file. Applied for and set up Berzelius project. Designed and created the project presentation.</p>
<h2 id="references">References</h2>
<p>[1] Schouten, Gerard, Bas SHT Michielsen, and Barbara Gravendeel. "Data-centric AI approach for automated wildflower monitoring." Plos one 19.9 (2024): e0302958. <br>
[2] Sun, Tao, Dongsheng Li, and Bao Wang. "Decentralized federated averaging." IEEE Transactions on Pattern Analysis and Machine Intelligence 45.4 (2022): 4289-4301. <br>
[3] Kirillov, Alexander, et al. "Segment anything." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. <br>
[4] Bianchi, Federico, et al. "Contrastive language-image pre-training for the italian language." arXiv preprint arXiv:2108.08688 (2021). <br>
[5] Grill, Jean-Bastien, et al. "Bootstrap your own latent-a new approach to self-supervised learning." Advances in neural information processing systems 33 (2020): 21271-21284. <br>
[6] Chen, Ting, et al. "A simple framework for contrastive learning of visual representations." International conference on machine learning. PMLR, 2020. <br>
[7] Loshchilov, I. &amp; Hutter, F. "SGDR: Stochastic Gradient Descent with Warm Restarts." arXiv preprint arXiv.1608.03983 (2017). <br></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = ".",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="js/base.js"></script>
        <script src="search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!--
MkDocs version : 1.6.1
Build Date UTC : 2024-12-20 13:18:06.064424+00:00
-->
